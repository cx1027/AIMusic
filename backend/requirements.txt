fastapi==0.115.7
uvicorn[standard]==0.30.6
python-multipart==0.0.9
sqlmodel>=0.0.22
sqlalchemy==2.0.36
psycopg[binary]==3.2.3
python-dotenv==1.0.1
pydantic==2.10.6
pydantic-settings==2.8.0
python-jose==3.3.0
bcrypt>=4.0.0
email-validator==2.2.0
redis==5.0.8
celery==5.4.0
sse-starlette==2.1.3
boto3==1.34.162
httpx==0.27.2
pytest==8.3.4
pytest-asyncio==0.24.0


# PyTorch with CUDA 12.8 (for Windows/Linux)
--extra-index-url https://download.pytorch.org/whl/cu128
torch==2.7.1+cu128; sys_platform == 'win32'
torchvision==0.22.1+cu128; sys_platform == 'win32'
torchaudio==2.7.1+cu128; sys_platform == 'win32'
# macOS arm64 (Apple Silicon): latest CPU/MPS wheels
torch>=2.9.1; sys_platform == 'darwin' and platform_machine == 'arm64'
torchaudio>=2.9.1; sys_platform == 'darwin' and platform_machine == 'arm64'
torchvision; sys_platform == 'darwin' and platform_machine == 'arm64'
# Other non-Windows platforms (Linux)
torch==2.10.0+cu128; sys_platform != 'win32' and sys_platform != 'darwin'
torchvision==0.25.0+cu128; sys_platform != 'win32' and sys_platform != 'darwin'
torchaudio==2.10.0+cu128; sys_platform != 'win32' and sys_platform != 'darwin'

# Core dependencies
transformers>=4.51.0,<4.58.0
diffusers
huggingface_hub>=0.20.0  # For Hugging Face Inference API (FLUX.1 Schnell)
Pillow>=10.0.0  # For image processing
# gradio==6.2.0
matplotlib>=3.7.5
scipy>=1.10.1
soundfile>=0.13.1
loguru>=0.7.3
einops>=0.8.1
accelerate>=1.12.0
fastapi>=0.110.0
uvicorn[standard]>=0.27.0
numba>=0.63.1
vector-quantize-pytorch>=1.27.15
torchcodec>=0.9.1; sys_platform != 'darwin' or platform_machine == 'arm64'
torchao
toml
modelscope

# Training dependencies (required for LoRA training)
peft>=0.18.0
lightning>=2.0.0
tensorboard>=2.0.0

# MLX dependencies (Apple Silicon native acceleration - macOS only)
# Provides significant speedup for LLM inference on Apple Silicon GPUs
mlx>=0.25.2; sys_platform == 'darwin' and platform_machine == 'arm64'
mlx-lm>=0.20.0; sys_platform == 'darwin' and platform_machine == 'arm64'

# nano-vllm dependencies
# triton-windows>=3.0.0,<3.4; sys_platform == 'win32'
# triton>=3.0.0; sys_platform != 'win32'
# flash-attn @ https://github.com/sdbds/flash-attention-for-windows/releases/download/2.8.2/flash_attn-2.8.2+cu128torch2.7.1cxx11abiFALSEfullbackward-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11' and platform_machine == 'AMD64'
# flash-attn; sys_platform != 'win32'
xxhash

# Local package - install with: pip install -e acestep/third_parts/nano-vllm
# nano-vllm
